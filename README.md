# AI Python Platform ğŸš€

Production-ready Python AI execution layer that replaces n8n workflows. This platform is designed to be called by an existing Node.js backend and runs scalable AI pipelines for documents, news, and summaries.

## ğŸ“‹ Overview

The AI Python Platform is a FastAPI-based microservice that processes AI workloads asynchronously using Celery workers. It supports multiple environments (sandbox, dev, prod) and is ready for deployment on Azure Container Apps.

### Key Features

- âœ… **FastAPI** - High-performance async API
- âœ… **Celery** - Distributed task queue with retry logic
- âœ… **Redis** - Message broker and result backend
- âœ… **MongoDB** - Document storage
- âœ… **Structured Logging** - JSON logs with job tracking
- âœ… **Environment-based Config** - Sandbox, dev, and prod support
- âœ… **Docker-ready** - Production containers included
- âœ… **Azure-ready** - Deployable to Azure Container Apps

## ğŸ—ï¸ Architecture

```text
â”‚  Node.js Backendâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST /jobs/*
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚ â—„â”€â”€â”€â”€â–ºâ”‚ Ingestion Layer (Sync) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                            â”‚
         â”‚ Enqueues task (Async)      â–¼
         â–¼                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   Pinecone   â”‚
â”‚  Redis (Broker) â”‚           â”‚ (Vector DB)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Workers  â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚   MongoDB    â”‚
â”‚ (News/Summary)  â”‚       â”‚   (Storage)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Flow

1. **Ingestion Layer (Synchronous)**:
   - Node.js backend sends a PDF URL to `/jobs/document`.
   - FastAPI downloads, cleans, chunks, and generates embeddings from the document.
   - Text chunks and metadata are upserted into **Pinecone** immediately.
   - FastAPI returns a `200 OK` success response with ingestion results.

2. **Analysis Layer (Asynchronous)**:
   - Node.js backend sends a request to `/jobs/summary` or `/jobs/news`.
   - FastAPI enqueues a task in **Redis** and returns a `job_id` (HTTP 202).
   - **Celery workers** process the task in the background.
   - Node.js backend polls `/jobs/{job_id}` for completion.

## ğŸ“ Project Structure

```
ai-python-platform/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI entrypoint
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ jobs.py               # Job intake endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ celery_app.py         # Celery configuration
â”‚   â”‚   â””â”€â”€ document_pipeline.py  # AI pipeline tasks
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ extraction.py         # Text extraction
â”‚   â”‚   â”œâ”€â”€ chunking.py           # Text chunking
â”‚   â”‚   â””â”€â”€ embedding.py          # Vector embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py             # Environment config
â”‚   â”‚   â””â”€â”€ logging.py            # Structured logging
â”‚   â”‚
â”‚   â””â”€â”€ db/
â”‚       â””â”€â”€ mongo.py              # MongoDB connection
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ api.Dockerfile            # API container
â”‚   â””â”€â”€ worker.Dockerfile         # Worker container
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11+
- Redis
- MongoDB
- (Optional) Docker

### Local Setup

1. **Clone and navigate to project**
   ```bash
   cd ai-python-platform
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

5. **Start Redis** (if not running)
   ```bash
   redis-server
   ```

6. **Start MongoDB** (if not running)
   ```bash
   mongod
   ```

### Running the Application

#### Option 1: Local Development

**Terminal 1 - Start API**
```bash
python3 -m app.main
# API available at http://localhost:8000
```

**Terminal 2 - Start Celery Worker**
```bash
celery -A app.workers.celery_app worker --loglevel=info
```

#### Option 2: Docker

**Build images**
```bash
docker build -f docker/api.Dockerfile -t ai-platform-api .
docker build -f docker/worker.Dockerfile -t ai-platform-worker .
```

**Run with docker-compose** (create docker-compose.yml first)
```bash
docker-compose up
```

## ğŸ”Œ API Endpoints

### Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "environment": "sandbox",
  "version": "1.0.0"
}
```

### Submit Document Job
```http
POST /jobs/document
Content-Type: application/json

{
  "file_url": "https://example.com/document.pdf",
  "file_type": "pdf",
  "metadata": {
    "source": "user_upload"
  }
}
```

**Response (HTTP 202):**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "accepted",
  "message": "Document processing job enqueued successfully"
}
```

### Submit News Job
```http
POST /jobs/news
Content-Type: application/json

{
  "article_url": "https://example.com/article",
  "metadata": {}
}
```

### Submit Summary Job
```http
POST /jobs/summary
Content-Type: application/json

{
  "text": "Long text to summarize...",
  "summary_type": "brief"
}
```

### Check Job Status
```http
GET /jobs/{job_id}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "state": "SUCCESS",
  "result": {
    "chunk_count": 5,
    "char_count": 1234,
    "execution_time": 2.5
  }
}
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `APP_ENV` | Environment (sandbox/dev/prod) | `sandbox` |
| `DEBUG` | Debug mode | `false` |
| `API_PORT` | API port | `8000` |
| `REDIS_HOST` | Redis host | `localhost` |
| `REDIS_PORT` | Redis port | `6379` |
| `MONGO_URI` | MongoDB connection string | `mongodb://localhost:27017` |
| `LOG_LEVEL` | Logging level | `INFO` |

See `.env.example` for complete configuration.

## ğŸ“Š Logging

All logs are structured JSON with the following fields:

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "info",
  "event": "job_start",
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "pipeline": "document_pipeline",
  "environment": "prod",
  "execution_time": 2.5
}
```

## ğŸ³ Docker Deployment

### Build for Production
```bash
docker build -f docker/api.Dockerfile -t ai-platform-api:latest .
docker build -f docker/worker.Dockerfile -t ai-platform-worker:latest .
```

### Push to Registry
```bash
docker tag ai-platform-api:latest <registry>/ai-platform-api:latest
docker push <registry>/ai-platform-api:latest

docker tag ai-platform-worker:latest <registry>/ai-platform-worker:latest
docker push <registry>/ai-platform-worker:latest
```

## â˜ï¸ Azure Container Apps Deployment

1. **Create Azure resources**
   - Container Apps Environment
   - Redis Cache
   - CosmosDB (MongoDB API) or MongoDB Atlas

2. **Configure environment variables** in Azure Container Apps

3. **Deploy API container**
   ```bash
   az containerapp create \
     --name ai-platform-api \
     --resource-group <rg> \
     --environment <env> \
     --image <registry>/ai-platform-api:latest \
     --target-port 8000 \
     --ingress external
   ```

4. **Deploy Worker container**
   ```bash
   az containerapp create \
     --name ai-platform-worker \
     --resource-group <rg> \
     --environment <env> \
     --image <registry>/ai-platform-worker:latest \
     --ingress internal
   ```

## ğŸ”„ Celery Worker Management

### Start Worker
```bash
celery -A app.workers.celery_app worker --loglevel=info
```

### Monitor Tasks
```bash
celery -A app.workers.celery_app events
```

### Purge Queue (Development only)
```bash
celery -A app.workers.celery_app purge
```

## ğŸ§ª Testing

From your Node.js backend:

```javascript
const response = await fetch('http://localhost:8000/jobs/document', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    file_url: 'https://example.com/doc.pdf',
    file_type: 'pdf'
  })
});

const { job_id } = await response.json();

// Poll for result
const statusResponse = await fetch(`http://localhost:8000/jobs/${job_id}`);
const status = await statusResponse.json();
```

## ğŸ“ Development Workflow

1. **Make code changes** in `app/`
2. **Test locally** with hot reload (DEBUG=true)
3. **Commit changes** to Git
4. **Build Docker images** for deployment
5. **Deploy to environment** (sandbox â†’ dev â†’ prod)

## ğŸ› ï¸ Troubleshooting

### Workers not processing tasks
- Check Redis connection
- Verify Celery broker URL
- Check worker logs

### API not responding
- Check FastAPI logs
- Verify port 8000 is not in use
- Check MongoDB connection

### MongoDB connection failed
- Ensure MongoDB is running
- Check `MONGO_URI` configuration
- Verify network connectivity

## ğŸ“š Next Steps

- [ ] Implement actual AI models (embeddings, summarization)
- [ ] Add authentication/API keys for Node.js backend
- [ ] Set up monitoring (Prometheus, Grafana)
- [ ] Add rate limiting
- [ ] Implement result webhooks
- [ ] Add comprehensive tests

## ğŸ“„ License

Proprietary - Internal Use Only

---

**Built with â¤ï¸ for scalable AI workloads**